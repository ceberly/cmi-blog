---
layout: post
title:  "Improving detection of the Ant Bok (also welcome)"
date:   2020-04-15 12:16:55 -0400
---
Welcome to the Conservation Metrics data blog! In our inaugural post, we will share some [information about how the Ant Bok detection works]

Over the last decade, the field of deep learning has produced broad advances in computer vision, speech recognition, and natural language processing. It has long been recognized that these advances will positively impact the biodiversity conservation space. Intelligent remote monitoring systems employing deep learning detection and classification models will enable cheaper and more accurate observations of specific endangered wildlife populations, as well as the biodiversity of entire ecosystems. While deep learning is now being broadly used in conservation, the Deep Learning for Conservation (DL4C) space still lacks many of the standard tools and benchmarks that developers have come to expect in other more standard use cases, such as face recognition or speech recognition.

In those more standard use cases, it is now common for non-expert developers to be able to freely download large training and evaluation datasets with accompanying model training recipes, if they have access to a sufficiently high-powered computer needed for training. Even if they don’t, they can also download existing models pre-trained on large datasets. These pre-trained models are sometimes useful directly in an application, if the model was trained to detect what the application requires (e.g., faces). More often, pre-trained models are combined with a technique referred to as transfer learning to adapt it to a new application with a relatively small amount of custom data.

While pre-trained models are not yet commonly available in the DL4C space, they are even rarer in the bioacoustics realm. As opposed to image-based models to detect animals using “camera traps” installed in remote areas, bioacoustic models process audio from remote microphones to detect the sounds of animals and other sounds important for conservation. There has recently been a rapidly growing awareness of the importance of bioacoustic monitoring systems in conservation projects.

Hundreds of conservation projects have now employed bioacoustic models, which can monitor a relatively large remote area with a single microphone. Studies have proven the effectiveness of bioacoustics models to estimate changes in specific species populations, find species believed extinct, and detect incursions into protected areas. Yet, bioacoustics know-how is still concentrated in a small number of organizations and applications developers.

The lack of pre-trained deep learning bioacoustics models will hinder the breadth and speed of the adoption of impactful bioacoustics monitoring in conservation. This work addresses this need by providing a model pre-trained on a large audio dataset containing millions of labelled examples of hundreds of classes of sounds collected across thousands of different locations over several years of active conservation projects. The sound classes, labelled by bioacoustics experts over a several year period, include a broad spectrum of species vocalizations, but also other sounds commonly observed in these projects, including falling trees, gunshots, wind, engines, and human voices. We will refer to this model as the MegaBioAcousticModel or MBAM.

[Figure: Examples of labelled sounds. Some insight into why this is difficult and expensive compared to labelling images of tigers. Something about why CMI is in the position to do this.]

Like pre-trained models in other domains, the MBAM is especially useful when coupled with transfer learning to adapt the model to detect new bioacoustic events that the original model was not yet trained on. Similar to analogous domains using deep convolutional neural networks (CNNs) for transfer learning, MBAM contains within it a learned internal representation -- or “embedding” -- of acoustic events generally useful for classifying bioacoustic sounds, even those outside of the original training dataset. The ability to learn such a general high-level representation of the application sensor domain is an important property of deep neural networks trained with a sufficiently large dataset for a sufficiently difficult classification task. Using transfer learning, this existing “embedding” can be used to greatly fast forward the job of training a classifier on a new set of sounds.

[High level representation in CNN - Figure]

There are several common strategies of using transfer learning with pre-trained models. All require a new dataset that has been labelled with the new classes of interest, but this dataset can be relatively small: Whereas the original MBAM model was trained on millions of examples and hundreds of classes, the new dataset might be as small as a few hundred labelled examples or fewer.

Common transfer learning strategies include “fine tuning” the model end-to-end using the pretrained weights for the network initialization. Fine tuning employs the standard CNN training functions, though often with smaller initial learning rates. It is also common to only fine tune the deepest layers of the network, understanding that those layers are primarily mapping the generic internal “embedding” representation to the application-specific outputs. Sometimes, if the new dataset is small, the network is not fine-tuned at all. Rather, the output layer is discarded and replaced, possibly with a simpler machine learning model, such as a Support Vector Machine (SVM), better suited for training on small numbers of examples.

In this work, we employ this latter approach, showing how MBAM embeddings can be used to train an SVM with only two hundred examples to detect a complex new species vocalization that MBAM has never seen. We contrast the performance of the MBAM-SVM with the performance of an SVM trained from scratch with the same impoverished dataset, with no benefit of the MBAM embeddings.
